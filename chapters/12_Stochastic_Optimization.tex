\section{Stochastic Optimization}
The \textbf{stochastic optimization problem} is $\min_{\xx \in X}{F(\xx)}$ with $F(\xx) = \E_{\xxi}[f(\xx, \xxi)]$, where $f(\xx, \xxi)$ is a function involving the decision variable $\xx$ and a random variable (vector) $\xxi$. The random variable $\xxi$ is some well defined variable with support $\Xi \subseteq \R^m$ and follows the distribution $P(\xxi)$. If $\xxi$ is the uniform distribution over the index set $\{1, \dots, n\}$, then $F(\xx) = \frac{1}{n}\sum_{i=1}^n{f_i(\xx)}$, this is the \textbf{finite-sum problem}. \\
\Title{Stochastic Gradient Descent}
Assume that $F(\xx, \xxi)$ is a continuously differentiable for any realization $\xxi \in \Xi$. We update $\xx_{t+1}$ as follows: $\xx_{t+1} := \Pi_X(\xx_t - \gamma_t \nabla f(\xx_t, \xxi_t))$, where $\xxi_t \sim P(\xxi)$, i.i.d. Here the gradient is taken over the argument $\xx$. In the finite-sum case we get: $\xx_{t+1} = \Pi_X(\xx_t - \gamma_t \nabla f_{i_t}(\xx_t))$ where $i_t$ is sampled uniformly at random from $\{1, \dots, n\}$. \\
We also assume that the stochastic gradient is unbiased i.e. $\E[\nabla f(\xx, \xxi)] = \nabla F(\xx)$. \\
\textbf{Remark}: We need $\gamma_t \rightarrow 0$ for $t \rightarrow \infty$ to ensure convergence. \\
\textbf{Convergence for strongly convex functions}: Assume that $F(\xx)$ is $\mu$-strongly convex and $\exists M > 0$, such that $\E[\norm{\nabla f(\xx, \xxi)}_2^2] \leq M^2$, $\forall \xx \in X$, then with stepsize $\gamma_t = \frac{\gamma}{t}$, with $\gamma \geq \frac{1}{2\mu}$ we get : $\E[\norm{\xx_t - \xx^*}_2^2]$ where $\frac{C(\gamma)}{t}$. \\
\textbf{Stochastic Mirror Descent}, works as follows: $x_{t+1} := \argmin_{\xx \in X} \{V_{\omega}(\xx, \xx_t) + \langle \gamma_t G(\xx_t, \xxi), \xx \rangle\}$, where for a given input $\xx, \xxi$ the estimator $G(x, \xxi)$ satisfies that $\E[G(\xx, \xxi)] \in \partial F(\xx)$ and $\E[\norm{G(\xx, \xxi)}_*^2] \leq M^2$. Note that we don't require $F(\xx)$ or $f(\xx, \xxi)$ to be differentiable. \\
\textbf{Convergence for convex functions} Let $F$ be convex, then stochastic gradient descent satisfies that $\E[F(\hat{\xx_T} - F(\xx^*)] \leq \frac{R^2 + \frac{M^2}{2}\sum_{t=1}^T{\gamma_t^2}}{\sum_{t=1}^T{\gamma_t}}$, with $R^2 = \max_{\xx \in X}{V_{\omega}(\xx, \xx_1)}$ and $\hat{\xx_T}=\frac{\sum_{t=1}^T{\gamma_t \xx_t}}{\sum_{t=1}^T{\gamma_t}}$.\\
\textbf{Convergence of SGD under constant stepsize}: Assume that $F(\xx)$ is both $\mu$-strongly convex and $L$-smooth. Moreover assume that stochastic gradient satisfies that $\E[\norm{\nabla f(\xx, \xxi)}_2^2] \leq \sigma^2 + c \norm{\nabla F(\xx)}_2^2$, then SGD with constant stepsize $\gamma$ satisfies: $\E[F(\xx_t) - F(\xx^*)] \leq \frac{\gamma L \sigma^2}{2\mu} + (1 - \gamma \mu)^{t-1}[F(\xx_1) - F(\xx^*)]$, where $\xx^*$ is the optimal solution. \\
The condition on the gradient can be viewed as a generalization of the bounded variance assumption (which we recover if $c=1$. If $\sigma^2 = 0$, we have a \textbf{strong growth condition} with constant $c$. If $\sigma^2 =0$ and $c=1$ we recover the deterministic setting. \\
\Title{Adaptive Stochastic Gradient Methods} 
Adaptive gradient methods, are methods whose stepsizes and search directions are adjusted based on past gradients. \\
\textbf{General Framework} For $t =1, \dots, T$ we successively define $\vecg_t = \nabla f(\xx_t, \xxi_t)$, $m_t = \phi(\vecg_1, \dots, \vecg_t)$, $V_t = \psi(\vecg_1, \dots, \vecg_t)$, for some functions $\phi, \psi$ to be specified $\widehat{\xx_t} = \xx_t - \alpha V_t^{-1/2} \mm_t$ and $x_{t+1} = \argmin_{\xx \in X} \{(\xx - \widehat{\xx_t})^T V_t^{1/2}(\xx - \widehat{\xx_t})\}$. For example:
\textbf{AdaGrad}: AdaGrad rescales the learning rate component-wise by the square root of the cumulative sum of the previous gradients: $\vv_t = \vv_{t-1} + \nabla f(\xx_t, \xxi_t)^{\odot 2}$ and $\xx_{t+1} = \xx_t - \frac{\gamma_0}{\epsilon + \sqrt{\vv_t}}\odot \nabla f(\xx_t, \xxi_t)$, with $\odot$ component-wise product. \\
\textbf{RMSProp}: RMSProp uses a moving average of the squared gradients with a discount factor to slow down the decay of the learning rates: $\vv_t = \beta \vv_{t-1} + (1 - \beta) \nabla f(\xx_t, \xxi_t)^{\odot 2}$ and $\xx_{t+1} = \xx_t - \frac{\gamma_0}{\epsilon + \sqrt{\vv_t}}\odot \nabla f(\xx_t, \xxi_t)$. $\beta \in (0, 1)$ is chosen close to 1. \\
\textbf{Adam}: Adam combines RMSProp with Momentum estimation. Similar to RMSProp, Adam also keeps an exponentially decaying average of past gradients, similar to the momentum estimation. Because
of the factor $\beta_1$ , $\beta_2$ , the estimates $m_t$ and $\vv_t$ of the first and second moments of the gradient become biased, Adam also counteract these biases by normalizing these terms. $\vv_t = \beta_2 \vv_{t-1} + (1 - \beta_2) \nabla f(\xx_t, \xxi_t)^{\odot 2}$, $\mm_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla f(\xx_t, \xxi_t)$, $\xx_{t+1} = \xx_t - \frac{\gamma_0}{\epsilon + \sqrt{\Tilde{\vv}_t}}\cdot \Tilde{\mm}_t$, here $\Tilde{\vv}_t = \frac{\vv_t}{1 - \beta^t}$ and $\Tilde{m}_t = \frac{m_t}{1 - \alpha^t}$ are bias corrected.